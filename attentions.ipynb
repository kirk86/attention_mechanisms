{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch # type: ignore\n",
    "import numpy as np # type: ignore\n",
    "import matplotlib.pyplot as plt # type: ignoreb\n",
    "\n",
    "\n",
    "nn = torch.nn\n",
    "F = torch.nn.functional"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dot product attention, also known as softmax attention, or flash-attention (cuda version)",
    "Flash Attention:",
    "- https://arxiv.org/abs/2407.08608",
    "- https://github.com/dao-ailab/flash-attention",
    "- https://www.stephendiehl.com/posts/flash_attention/",
    "- DeepSeek MultiHeadLatentAttention (Groupped Attention)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$Attention(X) = Softmax(\\frac{Q \\cdot K^{\\top}}{\\sqrt(d_{k})} + M) \\cdot V$\n",
    "\n",
    "\n",
    "$d_{k} = Q.size(-1) \\rightarrow$ head dimension $\\rightarrow$ number of heads x head dimension = embedding dimension\n",
    "\n",
    "e.g., if embedding dimension = 64 then we have 2 options 2 heads x 32 head dimension, or, 8 heads x 8 head dimension\n",
    "\n",
    "$Q = X \\cdot W_{q}$\n",
    "\n",
    "$K = X \\cdot W_{k}$\n",
    "\n",
    "$V = X \\cdot W_{v}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dot_prod_attention(query, key, value, mask=None, dropout=None):\n",
    "    \"Compute 'Scaled Dot Product Attention'\"\n",
    "    # pdb.set_trace()\n",
    "    d_k = query.size(-1)\n",
    "    scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k)\n",
    "    if mask is not None:\n",
    "        \"\"\"\n",
    "        mask = [[0, -inf, -inf],\n",
    "                [0, 0, -inf],\n",
    "                [0, 0, 0]]     \n",
    "        \"\"\" \n",
    "        scores = scores + mask\n",
    "    p_attn = F.softmax(scores, dim=-1)\n",
    "    if dropout is not None:\n",
    "        p_attn = dropout(p_attn)\n",
    "    return torch.matmul(p_attn, value), p_attn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def linformer_attention(Q, K, V, E, mask):\n",
    "    head_dim = Q.size(-1)\n",
    "    K = torch.matmul(E, K * mask[:, None, :, None])\n",
    "    V = torch.matmul(E, V * mask[:, None, :, None])\n",
    "    dot = torch.matmul(Q, torch.transpose(K, -2, -1))\n",
    "    dot = dot / math.sqrt(head_dim)\n",
    "    attn = F.softmax(dot, dim=-1)\n",
    "    X = torch.matmul(attn, V)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aft_attention(self, x, d_model,  n=49, simple=False):\n",
    "    \"\"\"Attention Free Transformer\"\"\"\n",
    "    B, N, D = x.shape\n",
    "    if simple:\n",
    "        self.position_biases = torch.zeros((n, n))\n",
    "    else:\n",
    "        self.position_biases = nn.Parameter(torch.ones((n, n)))\n",
    "    q = nn.Linear(d_model, d_model)(x)\n",
    "    k = nn.Linear(d_model, d_model)(x).view(1, B, N, D)\n",
    "    v = nn.Linear(d_model, d_model)(x).view(1, B, N, D)\n",
    "    numerator = torch.sum(torch.exp(k + self.position_biases.view(N, 1, -1, 1)) * v, dim=2)\n",
    "    denominator = torch.sum(torch.exp(k + self.position_biases.view(N, 1, -1, 1)), dim=2)\n",
    "    out = numerator / denominator\n",
    "    out = torch.sigmoid(q) * (out.permute(1, 0, 2))\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch  # type: ignore\n",
    "from src.components.utils import clones\n",
    "# from transformers.modeling_reformer import LSHSelfAttention, ReformerConfig\n",
    "\n",
    "\n",
    "nn = torch.nn\n",
    "F = torch.nn.functional\n",
    "\n",
    "\n",
    "class SoftmaxAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.drop_attn = torch.nn.Dropout(p=config[\"dropout_prob\"])\n",
    "        self.head_dim = config[\"head_dim\"]\n",
    "\n",
    "    def forward(self, Q, K, V, mask):\n",
    "        dot = torch.matmul(Q, torch.transpose(K, -2, -1))\n",
    "        dot = dot / math.sqrt(self.head_dim)\n",
    "        dot = dot - 1e6 * (1 - mask[:, None, None, :])\n",
    "\n",
    "        attn = F.softmax(dot, dim=-1)\n",
    "        attn = self.drop_attn(attn)\n",
    "\n",
    "        X = torch.matmul(attn, V)\n",
    "        return X\n",
    "\n",
    "\n",
    "class MultiHeadedAttention(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        h,\n",
    "        d_model,\n",
    "        dropout=0.1,\n",
    "        bias=True,\n",
    "        freeze_q=False,\n",
    "        freeze_k=False,\n",
    "        freeze_v=False,\n",
    "        zero_k=False,\n",
    "    ):\n",
    "        \"Take in model size and number of heads.\"\n",
    "        super(MultiHeadedAttention, self).__init__()\n",
    "        self.bias = bias\n",
    "        assert d_model % h == 0\n",
    "        # We assume d_v always equals d_k\n",
    "        self.d_k = d_model // h\n",
    "        self.h = h\n",
    "        self.linears = clones(nn.Linear(d_model, d_model, bias=bias), 4)\n",
    "        if freeze_q:\n",
    "            self.linears[0].requires_grad_(False)\n",
    "        if freeze_k:\n",
    "            self.linears[1].requires_grad_(False)\n",
    "        if freeze_v:\n",
    "            self.linears[2].requires_grad_(False)\n",
    "        if zero_k:\n",
    "            self.null_linear_layer(self.linears[1])\n",
    "        self.attn = None\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    def null_linear_layer(self, ln):\n",
    "        with torch.no_grad():\n",
    "            ln.weight.fill_(0.0)\n",
    "            if self.bias:\n",
    "                ln.bias.fill_(0.0)\n",
    "        ln.requires_grad_(False)\n",
    "\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        \"Implements Figure 2\"\n",
    "        if mask is not None:\n",
    "            # Same mask applied to all h heads.\n",
    "            mask = mask.unsqueeze(0).unsqueeze(0)\n",
    "        nbatches = query.size(0)\n",
    "\n",
    "        # 1) Do all the linear projections in batch from d_model => h x d_k\n",
    "        query, key, value = [\n",
    "            layer(x).view(nbatches, -1, self.h, self.d_k).transpose(1, 2)\n",
    "            for layer, x in zip(self.linears, (query, key, value))\n",
    "        ]\n",
    "\n",
    "        # 2) Apply attention on all the projected vectors in batch.\n",
    "        x, self.attn = attention(query, key, value, mask=mask, dropout=self.dropout)\n",
    "\n",
    "        # 3) \"Concat\" using a view and apply a final linear.\n",
    "        x = x.transpose(1, 2).contiguous().view(nbatches, -1, self.h * self.d_k)\n",
    "        return self.linears[-1](x)\n",
    "\n",
    "\n",
    "class LinformerAttention(nn.Module):\n",
    "    projection_matrix = None\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_head = config[\"num_head\"]\n",
    "        self.head_dim = config[\"head_dim\"]\n",
    "        self.linformer_k = config[\"linformer_k\"]\n",
    "        self.seq_len = config[\"max_seq_len\"]\n",
    "\n",
    "        if LinformerAttention.projection_matrix is not None:\n",
    "            self.E = LinformerAttention.projection_matrix\n",
    "        else:\n",
    "            LinformerAttention.projection_matrix = nn.Parameter(torch.Tensor(self.num_head, self.linformer_k, self.seq_len))\n",
    "            nn.init.normal_(LinformerAttention.projection_matrix, std=0.02)\n",
    "            self.E = LinformerAttention.projection_matrix\n",
    "\n",
    "    def forward(self, Q, K, V, mask):\n",
    "        K = torch.matmul(self.E, K * mask[:, None, :, None])\n",
    "        V = torch.matmul(self.E, V * mask[:, None, :, None])\n",
    "\n",
    "        dot = torch.matmul(Q, torch.transpose(K, -2, -1))\n",
    "        dot = dot / math.sqrt(self.head_dim)\n",
    "\n",
    "        attn = F.softmax(dot, dim=-1)\n",
    "\n",
    "        X = torch.matmul(attn, V)\n",
    "\n",
    "        return X\n",
    "\n",
    "\n",
    "class NystromAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "\n",
    "        self.head_dim = config[\"head_dim\"]\n",
    "        self.num_head = config[\"num_head\"]\n",
    "\n",
    "        self.num_landmarks = config[\"num_landmarks\"]\n",
    "        self.seq_len = config[\"seq_len\"]\n",
    "\n",
    "        if \"inv_coeff_init_option\" in config:\n",
    "            self.init_option = config[\"inv_init_coeff_option\"]\n",
    "        else:\n",
    "            self.init_option = \"original\"\n",
    "\n",
    "        self.use_conv = \"conv_kernel_size\" in config\n",
    "        if self.use_conv:\n",
    "            self.conv = nn.Conv2d(\n",
    "                in_channels=self.num_head,\n",
    "                out_channels=self.num_head,\n",
    "                kernel_size=(config[\"conv_kernel_size\"], 1),\n",
    "                padding=(config[\"conv_kernel_size\"] // 2, 0),\n",
    "                bias=False,\n",
    "                groups=self.num_head\n",
    "            )\n",
    "\n",
    "    def forward(self, Q, K, V, mask):\n",
    "\n",
    "        Q = Q * mask[:, None, :, None] / math.sqrt(math.sqrt(self.head_dim))\n",
    "        K = K * mask[:, None, :, None] / math.sqrt(math.sqrt(self.head_dim))\n",
    "\n",
    "        if self.num_landmarks == self.seq_len:\n",
    "            attn = F.softmax(torch.matmul(Q, K.transpose(-1, -2)) - 1e9 * (1 - mask[:, None, None, :]), dim=-1)\n",
    "            X = torch.matmul(attn, V)\n",
    "        else:\n",
    "            Q_landmarks = Q.reshape(-1, self.num_head, self.num_landmarks, self.seq_len // self.num_landmarks, self.head_dim).mean(dim=-2)\n",
    "            K_landmarks = K.reshape(-1, self.num_head, self.num_landmarks, self.seq_len // self.num_landmarks, self.head_dim).mean(dim=-2)\n",
    "\n",
    "            kernel_1 = F.softmax(torch.matmul(Q, K_landmarks.transpose(-1, -2)), dim=-1)\n",
    "            kernel_2 = F.softmax(torch.matmul(Q_landmarks, K_landmarks.transpose(-1, -2)), dim=-1)\n",
    "            kernel_3 = F.softmax(torch.matmul(Q_landmarks, K.transpose(-1, -2)) - 1e9 * (1 - mask[:, None, None, :]), dim=-1)\n",
    "            X = torch.matmul(torch.matmul(kernel_1, self.iterative_inv(kernel_2)), torch.matmul(kernel_3, V))\n",
    "\n",
    "        if self.use_conv:\n",
    "            X += self.conv(V * mask[:, None, :, None])\n",
    "\n",
    "        return X\n",
    "\n",
    "    def iterative_inv(self, mat, n_iter=6):\n",
    "        I = torch.eye(mat.size(-1), device=mat.device)\n",
    "        K = mat\n",
    "\n",
    "        # The entries of K are positive and ||K||_{\\infty} = 1 due to softmax\n",
    "        if self.init_option == \"original\":\n",
    "            # This original implementation is more conservative to compute coefficient of Z_0. \n",
    "            V = 1 / torch.max(torch.sum(K, dim=-2)) * K.transpose(-1, -2)\n",
    "        else:\n",
    "            # This is the exact coefficient computation, 1 / ||K||_1, of initialization of Z_0, leading to faster convergence. \n",
    "            V = 1 / torch.max(torch.sum(K, dim=-2), dim=-1).values[:, :, None, None] * K.transpose(-1, -2)\n",
    "\n",
    "        for _ in range(n_iter):\n",
    "            KV = torch.matmul(K, V)\n",
    "            V = torch.matmul(0.25 * V, 13 * I - torch.matmul(KV, 15 * I - torch.matmul(KV, 7 * I - KV)))\n",
    "        return V\n",
    "\n",
    "    def extra_repr(self):\n",
    "        return f'num_landmarks={self.num_landmarks}, seq_len={self.seq_len}'\n",
    "\n",
    "\n",
    "# class LSHAttention(LSHSelfAttention):\n",
    "#     \"\"\"\n",
    "#     LSH Attention - Reformer Attention\n",
    "#     \"\"\"\n",
    "#     def __init__(self, config, query, key, value):\n",
    "#         reformer_config = ReformerConfig()\n",
    "#         reformer_config.attn_layers = [\"lsh\"]\n",
    "#         reformer_config.num_hashes = config[\"num_hash\"]\n",
    "#         reformer_config.is_decoder = False\n",
    "#         reformer_config.max_position_embeddings = config[\"max_seq_len\"]\n",
    "#         reformer_config.hidden_size = config[\"transformer_dim\"]\n",
    "#         super().__init__(reformer_config)\n",
    "#         self.query_key.weight = query.weight\n",
    "#         self.value.weight = value.weight\n",
    "\n",
    "#     def forward(self, X, mask):\n",
    "#         return super().forward(hidden_states=X, attention_mask=mask).hidden_states\n",
    "\n",
    "\n",
    "class AFTAttention(nn.Module):  # AFT Attention\n",
    "    \"\"\"\n",
    "    AFT Attention\n",
    "    refer to \"An Attention Free Transformer\"\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, n=49, simple=False):\n",
    "        super().__init__()\n",
    "        self.fc_q = nn.Linear(d_model, d_model)\n",
    "        self.fc_k = nn.Linear(d_model, d_model)\n",
    "        self.fc_v = nn.Linear(d_model, d_model)\n",
    "        if simple:\n",
    "            self.position_biases = torch.zeros((n, n))\n",
    "        else:\n",
    "            self.position_biases = nn.Parameter(torch.ones((n, n)))\n",
    "        self.d_model = d_model\n",
    "        self.n = n\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, N, D = x.shape\n",
    "\n",
    "        q = self.fc_q(x)\n",
    "        k = self.fc_k(x).view(1, B, N, D)\n",
    "        v = self.fc_v(x).view(1, B, N, D)\n",
    "\n",
    "        numerator = torch.sum(torch.exp(k + self.position_biases.view(N, 1, -1, 1)) * v, dim=2)\n",
    "        denominator = torch.sum(torch.exp(k + self.position_biases.view(N, 1, -1, 1)), dim=2)\n",
    "\n",
    "        out = numerator / denominator\n",
    "        out = self.sigmoid(q) * (out.permute(1, 0, 2))\n",
    "        return out\n",
    "\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     dummy_input = ms.ops.randn((50, 49, 512))\n",
    "#     aft_full = AFT_FULL(d_model=512, n=49)\n",
    "#     output = aft_full(dummy_input)\n",
    "#     print(output.shape)\n",
    "\n",
    "\n",
    "# Relative Position Attention\n",
    "\"\"\"\n",
    "An illustrated explanation and code can be found at:\n",
    "https://nn.labml.ai/transformers/xl/relative_mha.html\n",
    "https://github.com/labmlai/annotated_deep_learning_paper_implementations/blob/master/labml_nn/transformers/xl/relative_mha.py\n",
    "https://github.com/evelinehong/Transformer_Relative_Position_PyTorch/blob/master/relative_position.py\n",
    "https://medium.com/@rajveer.rathod1301/relative-positional-multi-head-attention-an-overview-e7d22a63e01c\n",
    "\"\"\"\n",
    "# class RelativePosition(nn.Module):\n",
    "\n",
    "#     def __init__(self, num_units, max_relative_position):\n",
    "#         super().__init__()\n",
    "#         self.num_units = num_units\n",
    "#         self.max_relative_position = max_relative_position\n",
    "#         self.embeddings_table = Parameter(torch.Tensor(max_relative_position * 2 + 1, num_units)\n",
    "#         nn.init.xavier_uniform_(self.embeddings_table)\n",
    "\n",
    "#     def forward(self, length_q, length_k):\n",
    "#         range_vec_q = torch.arange(length_q)\n",
    "#         range_vec_k = torch.arange(length_k)\n",
    "#         distance_mat = range_vec_k[None, :] - range_vec_q[:, None]\n",
    "#         distance_mat_clipped = torch.clamp(distance_mat, -self.max_relative_position, self.max_relative_position)\n",
    "#         final_mat = distance_mat_clipped + self.max_relative_position\n",
    "#         final_mat = torch.LongTensor(final_mat).cuda()\n",
    "#         embeddings = self.embeddings_table[final_mat].cuda()\n",
    "\n",
    "#         return embeddings\n",
    "\n",
    "\n",
    "# self.relative_position_k = RelativePosition(i, self.d_k, max_relative_position)\n",
    "# self.relative_position_v = RelativePosition(i, self.d_v, max_relative_position)\n",
    "\n",
    "# r_q = q.permute(2, 0, 1, 3).contiguous().view(len_q, sz_b*n_head, d_k)\n",
    "# r_k = self.relative_position_k(len_q, len_k)\n",
    "# attn_2 = torch.matmul(r_q, r_k.transpose(1, 2)).transpose(0, 1)\n",
    "# attn_2 = attn_2.contiguous().view(sz_b, self.n_head, len_k, len_k)\n",
    "\n",
    "# r_v = self.relative_position_v(len_q, len_v)\n",
    "# weight = attn.permute(2, 0, 1, 3).contiguous().view(len_q, sz_b*n_head, len_k)\n",
    "# weight = torch.matmul(weight, r_v)\n",
    "# weight = weight.transpose(0, 1).contiguous().view(sz_b, self.n_head, len_q, d_v)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Switch Transformers like MoE, Routing Transformer in the FFN layer\n",
    "https://github.com/kyegomez/SwitchTransformers/blob/main/switch_transformers/model.py\n",
    "https://github.com/labmlai/annotated_deep_learning_paper_implementations/blob/master/labml_nn/transformers/switch/__init__.py\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "To read https://pytorch.org/blog/flexattention/\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# type = {}\n",
    "\n",
    "# with open(\"/model/config.json\", \"r\") as f:\n",
    "#     config = json.load(f)\n",
    "# model_config = config[\"model\"]\n",
    "\n",
    "# if model_config[\"attn_type\"] == \"softmax\":\n",
    "#     type[\"softmax\"] = SoftmaxAttention\n",
    "\n",
    "# elif model_config[\"attn_type\"] == \"nystrom\":\n",
    "#     from attention_nystrom import NystromAttention\n",
    "#     type[\"nystrom\"] = NystromAttention\n",
    "\n",
    "# elif model_config[\"attn_type\"] == \"reformer\":\n",
    "#     from attention_reformer import LSHAttention\n",
    "#     type[\"reformer\"] = LSHAttention\n",
    "\n",
    "# elif model_config[\"attn_type\"] == \"linformer\":\n",
    "#     from attention_linformer import LinformerAttention\n",
    "#     type[\"linformer\"] = LinformerAttention\n",
    "\n",
    "# else:\n",
    "#     raise Exception()\n",
    "\n",
    "\n",
    "# Config for NystromModel 512\n",
    "# type = {\n",
    "#     \"model_checkpoints\": \"/model/model\",\n",
    "#     \"data_folder\": \"/dataset\",\n",
    "#     \"glue_dataset_folder\": \"/glue\",\n",
    "#     \"wikihop_dataset_folder\": \"/wikihop\",\n",
    "#     \"model\": {\n",
    "#         \"mixed_precision\": true,\n",
    "#         \"attention_grad_checkpointing\": false,\n",
    "#         \"gelu_grad_checkpointing\": true,\n",
    "#         \"vocab_size\": 50265,\n",
    "#         \"num_sen_type\": 1,\n",
    "#         \"max_seq_len\": 512,\n",
    "#         \"embedding_dim\": 768,\n",
    "#         \"transformer_dim\": 768,\n",
    "#         \"transformer_hidden_dim\": 3072,\n",
    "#         \"num_layers\": 12,\n",
    "#         \"dropout_prob\": 0.1,\n",
    "#         \"num_head\": 12,\n",
    "#         \"head_dim\": 64,\n",
    "#         \"attn_type\": \"nystrom\",\n",
    "#         \"num_landmarks\": 512,\n",
    "#         \"seq_len\": 64,\n",
    "#         \"conv_kernel_size\": 33\n",
    "#     },\n",
    "#     \"pretraining_setting\": {\n",
    "#         \"batch_size\": 256,\n",
    "#         \"learning_rate\": 0.0001,\n",
    "#         \"warmup\": 0.01,\n",
    "#         \"batches_per_report\": 10,\n",
    "#         \"batches_per_epoch\": 5000,\n",
    "#         \"epoch\": 100,\n",
    "#         \"validate_batches_per_epoch\": 100\n",
    "#     },\n",
    "#     \"gpu_setting\": {\n",
    "#         \"inst_per_gpu\": 8\n",
    "#     }\n",
    "# }\n",
    "\n",
    "import math\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "from alibi.config import ALiBiConfig\n",
    "\n",
    "\n",
    "def get_relative_positions(seq_len: int) -> torch.tensor:\n",
    "    x = torch.arange(seq_len)[None, :]\n",
    "    y = torch.arange(seq_len)[:, None]\n",
    "    return x - y\n",
    "\n",
    "\n",
    "def get_alibi_slope(num_heads):\n",
    "    x = (2 ** 8) ** (1 / num_heads)\n",
    "    return (\n",
    "        torch.tensor([1 / x ** (i + 1) for i in range(num_heads)])\n",
    "        .unsqueeze(-1)\n",
    "        .unsqueeze(-1)\n",
    "    )\n",
    "\n",
    "\n",
    "class ALiBiMultiHeadAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    from https://github.com/jaketae/alibi/blob/main/alibi/attention.py\n",
    "    \"\"\"\n",
    "    def __init__(self, config: ALiBiConfig) -> None:\n",
    "        super().__init__()\n",
    "        self.causal = config.causal\n",
    "        self.num_heads = config.num_heads\n",
    "        self.scale = math.sqrt(config.d_model)\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "        self.register_buffer(\"m\", get_alibi_slope(self.num_heads))\n",
    "        self.kqv = nn.Linear(config.d_model, 3 * config.d_model, bias=False)\n",
    "        if config.causal:\n",
    "            self.register_buffer(\n",
    "                \"mask\", torch.tril(torch.ones(1, 1, config.max_len, config.max_len))\n",
    "            )\n",
    "\n",
    "    def forward(self, x: torch.tensor) -> torch.tensor:\n",
    "        batch_size, seq_len, _ = x.shape\n",
    "\n",
    "        key, query, value = self.kqv(x).chunk(3, dim=-1)\n",
    "        key = key.view(batch_size, seq_len, self.num_heads, -1).permute(0, 2, 3, 1)\n",
    "        # key.shape == (batch_size, num_heads, d_head, seq_len)\n",
    "        query = query.view(batch_size, seq_len, self.num_heads, -1).transpose(1, 2)\n",
    "        value = value.view(batch_size, seq_len, self.num_heads, -1).transpose(1, 2)\n",
    "        # qv.shape == (batch_size, num_heads, seq_len, d_head)\n",
    "\n",
    "        bias = (self.m * get_relative_positions(seq_len)).unsqueeze(0)\n",
    "        # bias.shape == (1, num_heads, seq_len, seq_len)\n",
    "\n",
    "        score = torch.matmul(query, key) / self.scale + bias\n",
    "        # score.shape == (batch_size, num_heads, seq_len, seq_len)\n",
    "\n",
    "        if self.causal:\n",
    "            score = score.masked_fill(\n",
    "                self.mask[:, :, :seq_len, :seq_len] == 0, float(\"-inf\")\n",
    "            )\n",
    "\n",
    "        attn = F.softmax(score, dim=-1)\n",
    "        out = torch.matmul(attn, value)\n",
    "        # out.shape == (batch_size, num_heads, seq_len, d_head)\n",
    "        out = out.transpose(1, 2).reshape(batch_size, seq_len, -1)\n",
    "        # out.shape == (batch_size, seq_len, d_model)\n",
    "        out = self.dropout(out)\n",
    "\n",
    "        return out"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
